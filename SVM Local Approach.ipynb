{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84a92c02-df7f-41c6-b1b4-b9bff1682438",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import the necessaries libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02cf925-ae08-40da-a2a9-90333d3481f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F, Row\n",
    "import numpy as np\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e780fe97-1b9f-4986-abc0-ab17ea9e3d9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a447037e-3c3e-423d-85aa-79de2999efad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/mnt/2024-team20/labelled_datasets_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa5050a1-a653-41ff-a857-5861b4495681",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create a pipeline\n",
    "\n",
    "This pipeline comprises several stages for processing text data:\n",
    "- **Word2Vec**: Converts lemmatized tokens into vectors of numerical features, facilitating machine learning on textual input.\n",
    "- **StringIndexer**: Encodes string sentiment labels into numerical indices, a necessary step for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e32035b-55ec-4dee-961e-6ba67e379962",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol='lemmatized_tokens', outputCol='features')\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol='sentiment', outputCol = 'label')\n",
    "\n",
    "encoding_pipeline = Pipeline(stages=[word2Vec, stringIndexer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a08a6a26-e217-4ef8-8a73-82f78d951295",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Split the data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983e78fe-d134-43d6-8561-28e967e08003",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6215e7a1-65f0-47c6-9002-6cbf457e9b61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fit the pipeline on the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49dfed4-e439-4c2b-954e-6c3d43cceecb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline = encoding_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "317ea37a-3927-47cc-97c5-4f4b875a5630",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Transform Train and Test Data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e17940ca-1690-4171-acbb-8c248cba5fae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_data_train = pipeline.transform(train_df).select(F.col('features'), F.col('label'))\n",
    "\n",
    "model_data_test = pipeline.transform(test_df).select(F.col('features'), F.col('label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10664007-473e-43f6-867c-ae05b8cec206",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Optimize Data Distribution by Partitioning the Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf834b49-0530-4c0d-a4e3-c653d139c66a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "model_data_train_rdd = model_data_train.rdd.repartition(18)\n",
    "\n",
    "print(model_data_train_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99ee3378-bdae-4772-ae3d-53b158c3a574",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define a Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a2e26d2-d0c0-471b-861c-7e96c54ed409",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_model(partition_iter):\n",
    "    partition_data = list(partition_iter)\n",
    "    X_train = np.array([row.features.toArray() for row in partition_data])\n",
    "    Y_train = np.array([row.label for row in partition_data])\n",
    "    svm_classifier = svm.LinearSVC(random_state=0)\n",
    "    model = svm_classifier.fit(X_train, Y_train)\n",
    "    return [model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6c443e6-acea-4984-b636-e1a2bc73e8cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Train the Model on Each Partition and Collect the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a92e74e-904e-4649-a60f-e51d20b0ddf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models = model_data_train_rdd.mapPartitions(build_model).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9832010-533f-4a55-a2b7-36d9fb55060e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define a Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73338847-7533-49fe-a27b-8a6d2cb0247d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def predict(instance):\n",
    "    inst_features = np.array(instance[:-1]).reshape(1, -1)\n",
    "    predictions = [model.predict(inst_features)[0] for model in models]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc4fc4d1-9caf-43cd-885e-2bcc621e4c76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define a Prediction Aggregation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75b564f5-2c6a-42f8-847b-758d088c4136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform(instance):\n",
    "    predictions = predict(instance)\n",
    "    prediction_mode = max(set(predictions), key=predictions.count)\n",
    "    return Row(**instance.asDict(), raw_prediction=float(prediction_mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a575c0-3114-46df-900a-baf9385955ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Estimate Model Accuracy on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd5275d-6518-47db-87ee-eadfa98ac88a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.676339445195674\n"
     ]
    }
   ],
   "source": [
    "pred_df = model_data_test.rdd.map(transform).toDF()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'label', predictionCol = 'raw_prediction', metricName = 'accuracy')\n",
    "\n",
    "accuracy = evaluator.evaluate(pred_df)\n",
    "\n",
    "print(f'Accuracy = {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "SVM Local Approach",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
