{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbc25efb-a4c6-41bc-a690-5e12807f19be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc838b8-3b97-4742-bf2a-60b4165084a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql import functions as F\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cae167cf-9f69-4d41-9595-bd0f5e6826d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Configure NLTK Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c22273c-8e5d-47de-aab8-980997acf5c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def setup_nltk_data_path():\n",
    "\n",
    "  nltk_data_path = '/dbfs/nltk_data'  \n",
    "\n",
    "  if nltk_data_path not in nltk.data.path:\n",
    "    \n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46f2ae98-84c1-4550-82af-588db2553d46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load and Filter Reddit Comments Data from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6371dbc5-ee67-42d4-823b-bdc1034eabd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('comment', StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).csv('/mnt/2024-team20/reddit comments/politics_comments.csv')\n",
    "\n",
    "df = df.filter(F.col('comment').isNotNull())\n",
    "\n",
    "df = df.distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25a4cc41-4e01-44ad-8b95-e2243be89422",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define Regular Expressions for Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c564c413-c086-4f49-b77a-2e80ac8c1108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_pattern(col: Column, pattern: str) -> Column:\n",
    "    return F.regexp_replace(col, pattern, \"\")\n",
    "\n",
    "url_pattern = r'http\\S+|www\\S+|https\\S+'\n",
    "handle_pattern = r'@\\w+'\n",
    "hashtag_pattern = r'#\\w+'\n",
    "numerical_pattern = r'\\d+'\n",
    "punctuation_pattern = r'^[^\\w\\s]+|[^\\w\\s]+$'\n",
    "emoji_pattern = \"[\" \\\n",
    "                u\"\\U0001F600-\\U0001F64F\" \\\n",
    "                u\"\\U0001F300-\\U0001F5FF\" \\\n",
    "                u\"\\U0001F680-\\U0001F6FF\" \\\n",
    "                u\"\\U0001F700-\\U0001F77F\" \\\n",
    "                u\"\\U0001F780-\\U0001F7FF\" \\\n",
    "                u\"\\U0001F800-\\U0001F8FF\" \\\n",
    "                u\"\\U0001F900-\\U0001F9FF\" \\\n",
    "                u\"\\U0001FA70-\\U0001FAFF\" \\\n",
    "                u\"\\U00002702-\\U000027B0\" \\\n",
    "                u\"\\U000024C2-\\U0001F251\" \\\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\" \\\n",
    "                \"]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8111bc0-e7bd-4c36-bc86-c7715ec3f1cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean Text Data by Removing URLs, Handles, Hashtags, and Other Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b51b79-93a3-4696-8660-cce25e76d1f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df.withColumn('clean_comments', F.col('comment'))\n",
    "df_clean = df_clean.withColumn('clean_comments', remove_pattern(F.col('clean_comments'), url_pattern))\n",
    "df_clean = df_clean.withColumn('clean_comments', remove_pattern(F.col('clean_comments'), handle_pattern))\n",
    "df_clean = df_clean.withColumn('clean_comments', remove_pattern(F.col('clean_comments'), hashtag_pattern))\n",
    "df_clean = df_clean.withColumn('clean_comments', remove_pattern(F.col('clean_comments'), numerical_pattern))\n",
    "df_clean = df_clean.withColumn('clean_comments', remove_pattern(F.col('clean_comments'), punctuation_pattern))\n",
    "df_clean = df_clean.withColumn('clean_comments', remove_pattern(F.col('clean_comments'), emoji_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e698279-73ba-410c-8cc1-794510d6ae99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter Reddit Comments with Insufficient Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c3296e3-eea1-4de6-80b8-38bcad670665",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.filter(F.size(F.split(df_clean.clean_comments, '\\\\s+')) > 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8874fac3-5b17-415c-9505-14f36d35995c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tokenize and Clean Reddit Comments Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e02026-8d42-448c-ae4d-3c70bd4e1333",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol = 'clean_comments', outputCol = 'tokens')\n",
    "\n",
    "df_tokens = tokenizer.transform(df_clean)\n",
    "\n",
    "df_tokens = df_tokens.withColumn('tokens_cleaned', F.expr(\"filter(tokens, x -> x IS NOT NULL AND x != '')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a981320e-f2b9-42d7-9a5d-b17b059996c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Removal of Unnecessary Comment by the Reddit Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1158cd1-2ea0-4f03-bb93-244a60b2171a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# remove unnecessary comments produced by reddit team\n",
    "df_tokens = df_tokens.filter(~array_contains(df_tokens['tokens_cleaned'], \"bot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b32667d-c429-4ba6-95d1-45dad0a95ba2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Remove Stop Words and Filter Empty Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8045256c-809b-4364-b12c-dd51b1a1986a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol = 'tokens_cleaned', outputCol = 'filtered_tokens')\n",
    "\n",
    "df_filtered = remover.transform(df_tokens)\n",
    "\n",
    "df_filtered = df_filtered.filter(F.size(F.col('filtered_tokens')) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8e598da-3327-4137-aa4e-56d09fc94fc3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Refine Tokens by Stripping Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d4aaa1-603e-4781-876c-1be75cb7b07b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_array(arr):\n",
    "\n",
    "    cleaned_arr = []\n",
    "\n",
    "    for item in arr:\n",
    "\n",
    "        item = item.strip(string.punctuation)\n",
    "\n",
    "        if item:\n",
    "\n",
    "            cleaned_arr.append(item)\n",
    "            \n",
    "    return cleaned_arr\n",
    "  \n",
    "clean_array_udf = udf(clean_array, ArrayType(StringType()))\n",
    "\n",
    "df_filtered = df_filtered.withColumn('filtered_tokens', clean_array_udf(F.col('filtered_tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c84e742b-1c0f-4e9c-8207-7e2d13155c8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter Comments to Retain Only Those with Sufficient Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38384d28-f449-43e2-aff7-fc2ae6cd79eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.filter(F.size(df_filtered['filtered_tokens']) >= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51a2679-62a9-45c0-93d5-da45ddd0f5d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apply Lemmatization to Standardize Token Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34562dd-8985-413a-9f00-a5e29078fc9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "\n",
    "  setup_nltk_data_path()\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "lemmatize_words_udf = udf(lemmatize_words, ArrayType(StringType()))\n",
    "\n",
    "df_lemmatized = df_filtered.withColumn('lemmatized_tokens', lemmatize_words_udf(F.col('filtered_tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae9b0aa3-5d19-4cc7-9c10-294174a7b34a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter Out Empty Token Lists After Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de4fa8b-c84f-4753-8109-3107ae9544e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_lemmatized = df_lemmatized.filter(F.size(F.col('lemmatized_tokens')) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31f5686f-8062-460e-992e-eb24f38b3c8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Remove Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5ecf14-b828-4eaf-8268-e809e2497bea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['clean_comments', 'tokens', 'tokens_cleaned', 'filtered_tokens']\n",
    "\n",
    "final_df = df_lemmatized.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d5ef717-b0be-4328-94c2-dc0e8adfe6c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Export the Final Dataset to Parquet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c93b5dd-33bd-4340-b91a-b9eabb280866",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode('overwrite').parquet('/mnt/2024-team20/cleaned_reddit_comments_parquet')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Reddit Comments Preprocessing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
