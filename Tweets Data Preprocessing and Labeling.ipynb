{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "789b4741-b78e-4268-a6ef-9ae590882793",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5679e9db-c41e-44df-96b5-1e44ca65e70d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, StringIndexer\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "from pyspark.sql.column import Column\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46c1bee8-7beb-4c77-82ff-025d9ecad1d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Configure NLTK Data Path\n",
    "This cell defines the `setup_nltk_data_path` function, which ensures that the NLTK library has access to necessary data files stored in a specified directory. This setup is crucial for enabling NLTK functionalities such as tokenization, stemming, and sentiment analysis that depend on external data files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b1d16a-90da-4505-b14d-7327ff3f3135",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def setup_nltk_data_path():\n",
    "\n",
    "  nltk_data_path = '/dbfs/nltk_data'  \n",
    "  \n",
    "  if nltk_data_path not in nltk.data.path:\n",
    "    \n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042c7362-8ed0-4340-80ad-d5786cdddbee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define Regular Expressions for Text Cleaning\n",
    "\n",
    "This code cell defines a function `remove_pattern` to apply regular expressions for removing common text patterns like URLs, social media handles, hashtags, numeric data, punctuation, and emojis from text data. These patterns are essential for preprocessing text for machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da35b3fb-32d5-475e-9d5b-1fd4a5abc954",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_pattern(col: Column, pattern: str) -> Column:\n",
    "    return F.regexp_replace(col, pattern, \"\")\n",
    "\n",
    "url_pattern = r'http\\S+|www\\S+|https\\S+'\n",
    "handle_pattern = r'@\\w+'\n",
    "hashtag_pattern = r'#\\w+'\n",
    "numerical_pattern = r'\\d+'\n",
    "punctuation_pattern = r'^[^\\w\\s]+|[^\\w\\s]+$'\n",
    "emoji_pattern = \"[\" \\\n",
    "                u\"\\U0001F600-\\U0001F64F\" \\\n",
    "                u\"\\U0001F300-\\U0001F5FF\" \\\n",
    "                u\"\\U0001F680-\\U0001F6FF\" \\\n",
    "                u\"\\U0001F700-\\U0001F77F\" \\\n",
    "                u\"\\U0001F780-\\U0001F7FF\" \\\n",
    "                u\"\\U0001F800-\\U0001F8FF\" \\\n",
    "                u\"\\U0001F900-\\U0001F9FF\" \\\n",
    "                u\"\\U0001FA70-\\U0001FAFF\" \\\n",
    "                u\"\\U00002702-\\U000027B0\" \\\n",
    "                u\"\\U000024C2-\\U0001F251\" \\\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\" \\\n",
    "                \"]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e11e8bab-4bf1-4315-8836-37090c357b23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load and Filter Tweet Data from Parquet Files\n",
    "\n",
    "This cell reads the dataset containing tweets from a Parquet file using a predefined schema with PySpark. It also includes filtering to remove any rows where the 'tweet' field is null, ensuring that the dataset is ready for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2ab831b-c342-4907-8bea-fa9af0d48fc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('tweet', StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).parquet('/mnt/2024-team20/cleaned_datasets_parquet')\n",
    "\n",
    "df = df.filter(F.col('tweet').isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c66252-deb9-465d-bfb4-1f071b86f1ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean Text Data by Removing URLs, Handles, Hashtags, and Other Patterns\n",
    "\n",
    "This cell processes the tweet data by removing unwanted textual patterns such as URLs, social media handles, hashtags, numbers, punctuation, and emojis from each tweet. This cleaning is crucial for preparing the data for natural language processing tasks, improving the quality of the text analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb5b745-7051-48e4-a652-4839052311af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df.withColumn('clean_tweets', F.col('tweet'))\n",
    "\n",
    "df_clean = df_clean.withColumn('clean_tweets', remove_pattern(F.col('clean_tweets'), url_pattern))\n",
    "\n",
    "df_clean = df_clean.withColumn('clean_tweets', remove_pattern(F.col('clean_tweets'), handle_pattern))\n",
    "\n",
    "df_clean = df_clean.withColumn('clean_tweets', remove_pattern(F.col('clean_tweets'), hashtag_pattern))\n",
    "\n",
    "df_clean = df_clean.withColumn('clean_tweets', remove_pattern(F.col('clean_tweets'), numerical_pattern))\n",
    "\n",
    "df_clean = df_clean.withColumn('clean_tweets', remove_pattern(F.col('clean_tweets'), punctuation_pattern))\n",
    "\n",
    "df_clean = df_clean.withColumn('clean_tweets', remove_pattern(F.col('clean_tweets'), emoji_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c512993-c5e8-48df-8c30-ea07ca76309f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter Tweets with Insufficient Word Count\n",
    "\n",
    "This cell further refines the tweet dataset by filtering out tweets that contain fewer than four words. This step ensures that the remaining dataset only includes tweets with enough content to provide meaningful context for machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735368b9-1dc9-4ead-9e18-c5106bf45ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.filter(F.size(F.split(df_clean.clean_tweets, '\\\\s+')) > 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bac5051b-1549-4d51-84b3-244a036e920a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Tokenize and Clean Tweet Tokens\n",
    "\n",
    "This cell performs tokenization on the cleaned tweet data, splitting each tweet into individual words or 'tokens'. Following tokenization, it further cleans the tokens by removing any empty or null entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676faa5c-fe87-44a1-b922-eae63822d1b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol = 'clean_tweets', outputCol = 'tokens')\n",
    "\n",
    "df_tokens = tokenizer.transform(df_clean)\n",
    "\n",
    "df_tokens = df_tokens.withColumn('tokens_cleaned', F.expr(\"filter(tokens, x -> x IS NOT NULL AND x != '')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "924b1a40-2bf5-4322-aad1-0e80ba60e0af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Remove Stop Words and Filter Empty Tokens\n",
    "\n",
    "This cell removes commonly used stop words from the tokens to focus on more meaningful words in the tweets. Subsequently, any tweets that result in zero tokens after this removal are filtered out, ensuring that only tweets with meaningful content are retained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63465617-ae5f-45c9-8c6c-f5adde086a8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol = 'tokens_cleaned', outputCol = 'filtered_tokens')\n",
    "\n",
    "df_filtered = remover.transform(df_tokens)\n",
    "\n",
    "df_filtered = df_filtered.filter(F.size(F.col('filtered_tokens')) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13f3e55c-248d-4fe8-ac85-7bc67521d7b9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Refine Tokens by Stripping Punctuation\n",
    "\n",
    "This cell defines and applies a custom function to further clean the tokenized text by stripping any remaining punctuation. The function `clean_array` iterates over each token, removes surrounding punctuation, and keeps only non-empty tokens. This step is essential for ensuring that the tokens are clean and meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4587bf11-fe34-4bfa-ae04-3dfe29d1d82f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_array(arr):\n",
    "\n",
    "    cleaned_arr = []\n",
    "\n",
    "    for item in arr:\n",
    "\n",
    "        item = item.strip(string.punctuation)\n",
    "\n",
    "        if item:\n",
    "\n",
    "            cleaned_arr.append(item)\n",
    "\n",
    "    return cleaned_arr\n",
    "  \n",
    "clean_array_udf = udf(clean_array, ArrayType(StringType()))\n",
    "\n",
    "df_filtered = df_filtered.withColumn('filtered_tokens', clean_array_udf(F.col('filtered_tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef09c4ad-fe9a-41da-9b96-bd603eea0db9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter Tweets to Retain Only Those with Sufficient Content\n",
    "\n",
    "This cell enhances data quality by filtering out tweets with fewer than four tokens. This step ensures that only tweets with enough contextual information are retained for in-depth analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a6d6eb6-31f3-44df-a827-d412703e5609",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.filter(F.size(df_filtered['filtered_tokens']) >= 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62af7eca-80c2-4be3-b7e9-fec63edc2814",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apply Lemmatization to Standardize Token Forms\n",
    "\n",
    "This cell defines and applies a lemmatization function to the tokens to convert them into their base or dictionary forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c1ca52-fb6d-4b84-b46c-0991da577715",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "  \n",
    "  setup_nltk_data_path()\n",
    "  \n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "  return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "lemmatize_words_udf = udf(lemmatize_words, ArrayType(StringType()))\n",
    "\n",
    "df_lemmatized = df_filtered.withColumn('lemmatized_tokens', lemmatize_words_udf(F.col('filtered_tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f9a1ca-3565-4305-888d-678780c7dde0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter Out Empty Token Lists After Lemmatization\n",
    "\n",
    "This cell further refines the dataset by filtering out any tweets where the `lemmatized_tokens` column contains an empty list. This step ensures that only tweets with meaningful content (i.e., at least one valid, lemmatized word) are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123ff7e6-a191-4705-8f50-478a93f0014e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_lemmatized = df_lemmatized.filter(F.size(F.col('lemmatized_tokens')) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c69bf9-3df4-4752-b88f-24376d30b105",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define and Apply Sentiment Analysis Function to Lemmatized Tokens\n",
    "\n",
    "This cell creates a sentiment analysis function `analyze_sentiment` which uses NLTK's `SentimentIntensityAnalyzer` to evaluate the sentiment of lemmatized tokens. The function labels each tweet as 'positive', 'negative', or 'neutral' based on the computed sentiment score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f6f1458-1941-4dc6-8c5e-d2d9563345ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment(tokens):\n",
    "  \n",
    "  setup_nltk_data_path()\n",
    "\n",
    "  sid = SentimentIntensityAnalyzer(lexicon_file = '/dbfs/nltk_data/corpora/sentiment/vader_lexicon/vader_lexicon.txt')\n",
    "\n",
    "  scores = sid.polarity_scores(' '.join(tokens))\n",
    "\n",
    "  if scores['compound'] >= 0.05:\n",
    "\n",
    "    return 'positive'\n",
    "  \n",
    "  elif scores['compound'] <= -0.05:\n",
    "\n",
    "    return 'negative'\n",
    "  \n",
    "  else:\n",
    "\n",
    "    return 'neutral'  \n",
    "     \n",
    "analyze_sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "df_with_sentiment = df_lemmatized.withColumn('sentiment', analyze_sentiment_udf(F.col('lemmatized_tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1b8b2e5-18d9-4991-bfd7-34991b613a31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Remove Unnecessary Columns\n",
    "\n",
    "This cell identifies and removes columns that are no longer needed, including 'tweet', 'tokens', 'tokens_cleaned', and 'filtered_tokens'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4a3da6-2128-455f-a967-66a5385b4a5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['tweet', 'tokens', 'tokens_cleaned', 'filtered_tokens']\n",
    "\n",
    "final_df = df_with_sentiment.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d344ff2b-1fe1-4504-ab17-ff06935c73bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Export the Final Dataset to Parquet Format\n",
    "\n",
    "This cell saves the processed and labeled dataset to a Parquet file, overwriting any existing data at the specified location. Storing the data in Parquet format ensures efficient storage and quick access for future analysis, leveraging the columnar storage benefits to enhance data processing performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ccbfe3-9f94-42fe-977e-331776ecb42a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write.mode('overwrite').parquet('/mnt/2024-team20/labelled_datasets_parquet')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Tweets Data Preprocessing and Labeling",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
